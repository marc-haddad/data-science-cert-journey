---
title: "Balanced Accuracy and the F1 Score"
subtitle: "Machine Learning - Section $2.1.3$"
author: "_Marc Omar Haddad_"
date: "Created: 17 January, 2020"
output:
  html_notebook:
    theme: readable
    highlight: zenburn
  # pdf_document:
  # latex_engine: xelatex
  # df_print: kable
---
 <strong>Updated: `r format(Sys.time(), '%d %B, %Y')`</strong>
<!-- \begin{center}Updated: `r format(Sys.time(), '%d %B, %Y')`\end{center} -->
\

```{r dependencies, include=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(dslabs)
data(heights)
set.seed(2, sample.kind = "Rounding")
options(digits = 3)
y = heights$sex # Categorical outcome (male or female)
x = heights$height
# Var containing index vals
test_index = createDataPartition(y, times = 1, p = 0.5, list = FALSE)
# The actual splitting of the dataset by rand. gen. index vals.
train_set = heights[-test_index, ]
test_set = heights[test_index, ]
cutoff = seq(61, 70) # Defining sequence of cutoffs
accuracy = map_dbl(cutoff, function(x) {
  y_hat = ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  mean(y_hat == train_set$sex)
})
best_cutoff = cutoff[which.max(accuracy)]
y_hat = ifelse(test_set$height > best_cutoff, "Male", "Female") %>%
  factor(levels = levels(test_set$sex))
prev = mean(y == "Male")
```


As discussed previously, overall accuracy does not usually provide enough information to adequately assess an algorithm; hence the introduction of Sensitivity and Specificity. Indeed, a better (and more concise) metric than overall accuracy would be to find the **average** of Sensitivity and Specificity. This average is known as the **Balanced Accuracy**.  

Because Sensitivity and Specificity are rates, it is more appropriate to compute their *harmonic average*, like so:

$$ \frac{1}{\frac{1}{2}\left( \frac{1}{\mathrm{recall}}+\frac{1}{\mathrm{precision}} \right)}
$$

Remember: **Recall** $= \frac{TP}{(TP+FN)}$, **Precision** $=\frac{TP}{(TP + FP)}$  

\

The harmonic average above is also known as the **F1 Score**, and can be rewritten (for the sake of simplicity) like so:

$$2 \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
$$
\

### Other Considerations
Different contexts call for the prioritization to minimize some errors over others (e.g. preferring higher Sensitivity at the cost of lower Specificity and vice versa).  


Example: When it comes to plane safety, it is more important to maximize Sensitivity over Specificity. We assume **"Faulty"** to be the **"positive"** outcome:  


- The cost of grounding a plane that is *predicted* to be faulty, but is *actually* not faulty (i.e. **False Positive**), is much less costly than *predicting* a plane to be not faulty, but which is *actually* faulty and results in a crash (i.e. **False Negative**).  


Conversely, when it comes to capital murder cases, it is more important to maximize Specificity over Sensitivity. We assume **"Guilty"** to be the **"positive"** outcome:  


- When determining guiltiness, the cost of *predicting* a person to be not guilty when in fact they are *actually* guilty (i.e. **False Negative**), is much less costly than *predicting* a person to be guilty and sentencing them to death despite *actually* being not guilty (i.e. **False Positive**).

\

### Weighted F1 Score and `F_meas`


Depending on the context, the F1 Score can be weighed accordingly. We use $\beta$ to denote how much more important Sensitivity is to Specificity. Our weighted F1 Score formula looks like this:  

$$\frac{1}{\frac{\beta^2}{1+\beta^2} \frac{1}{\mathrm{recall}}+ \frac{1}{1+\beta^2}\frac{1}{\mathrm{precision}}}$$

\

In the `caret` package, the function `F_meas` returns a summary of the F1 Score, with $\beta$ defaulting to $1$.  

Knowing this, we can now reconstruct our earlier predictive algorithm (see Sections $2.1.1$ and $2.1.2$) to **maximize our F1 Score**:  

```{r f_1}
cutoff = seq(61, 70)
F_1 = map_dbl(cutoff, function(x) {
  y_hat = ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  F_meas(data = y_hat, reference = train_set$sex)
})
F_1
```

Let us now compare our F1 Score values with their respective cutoffs:

```{r f_1_plot, echo=FALSE}
data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff, F_1)) +
  geom_point() +
  geom_line()
```

```{r F_1_cutoff}
max(F_1)
best_cutoff = cutoff[which.max(F_1)]
best_cutoff
```


The maximum F1 Score is $0.614$, and is achieved by using a cutoff of $66$ inches. This is a more reasonable estimate than our previously obtained best cutoff of $64$ inches. Furthermore, if we analyze our new confusion matrix we can see that our Sensitivity and Specificity values are more balanced:

```{r new_confusion_matrix}
y_hat = ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
confusionMatrix(data = y_hat, reference = test_set$sex)
```







