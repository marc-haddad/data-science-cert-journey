---
title: "Local Weighted Regression (Loess)"
subtitle: "Machine Learning - Section $3.2.3$"
author: "_Marc Omar Haddad_"
date: "Published: 16 April, 2020 <br>"
output:
  html_notebook:
    theme: readable
    highlight: zenburn
  # pdf_document:
  #   latex_engine: xelatex
  #   df_print: kable
---
<!-- FOR UPDATES -->
  
  <!-- Uncomment if output = html_notebook: -->
    <!--  <center><strong>Updated: `r format(Sys.time(), '%d %B, %Y')`</strong></center> -->
  
  <!-- Uncomment if output = pdf_document: -->
    <!-- \begin{center}Updated: `r format(Sys.time(), '%d %B, %Y')`\end{center} -->

<!-- FOR UPDATES -->    

\

```{r dependencies, include=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(dslabs)
library(ggrepel)
data("polls_2008")
```


## Limitation of Bin Smoother Approach

\

A limitation of the earlier bin smoother approach (see previous notes) was that we were required to take very small windows for the approximately constant assumption to hold. This results in a small number of data points to average.

Local Weighted Regression (AKA Loess) permits us to consider larger windows and, thus, achieve more accurate, smoother trendlines.

\

## Mathematics of Loess

\

### Taylor's Theorem

If you look close enough at any smooth function $f(x)$, the relationship will appear **linear**. The previous bin smoother approach assumed that our function is approximately constant within a window, whereas now we shall assume the function is locally linear. This permits us to use larger window sizes without sacrificing accuracy.



### Formula

For our example we will start by using a **$3$ week window**. Mathematically, our line equation for **one** window is calculated as:

$$
E\left[Y_i\ \vert\ X_i=x_i\right] = \beta_0 + \beta_1(x_i - x_0) \\
\mathrm{if}\ \lvert{x_i-x_0}\rvert â‰¤ 10.5
$$

We assume $Y$ given $X$ to be a line within that window (as evidenced by the linear equation).
















