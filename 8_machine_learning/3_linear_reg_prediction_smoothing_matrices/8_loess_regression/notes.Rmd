---
title: "Local Weighted Regression (Loess)"
subtitle: "Machine Learning - Section $3.2.3$"
author: "_Marc Omar Haddad_"
date: "Published: 16 April, 2020 <br>"
output:
  html_notebook:
    theme: readable
    highlight: zenburn
  # pdf_document:
  #   latex_engine: xelatex
  #   df_print: kable
---
<!-- FOR UPDATES -->
  
  <!-- Uncomment if output = html_notebook: -->
    <!--  <center><strong>Updated: `r format(Sys.time(), '%d %B, %Y')`</strong></center> -->
  
  <!-- Uncomment if output = pdf_document: -->
    <!-- \begin{center}Updated: `r format(Sys.time(), '%d %B, %Y')`\end{center} -->

<!-- FOR UPDATES -->    

\

```{r dependencies, include=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(dslabs)
library(ggrepel)
data("polls_2008")
```


## Limitation of Bin Smoother Approach

\

A limitation of the earlier bin smoother approach (see previous notes) was that we were required to take very small windows for the approximately constant assumption to hold. This results in a small number of data points to average.

Local Weighted Regression (AKA Loess) permits us to consider larger windows and, thus, achieve more accurate, smoother trend lines.

\

## Mathematics of Loess

\

### Taylor's Theorem

If you look close enough at any smooth function $f(x)$, the relationship will appear **linear**. The previous bin smoother approach assumed that our function is approximately constant within a window, whereas now we shall assume the function is locally linear. This permits us to use larger window sizes without sacrificing accuracy.



### Formula

For our example we will start by using a **$3$ week window**. Mathematically, our line equation for **one** window is calculated as:

$$
E\left[Y_i\ \vert\ X_i=x_i\right] = \beta_0 + \beta_1(x_i - x_0) \\
\mathrm{if}\ \lvert{x_i-x_0}\rvert â‰¤ 10.5
$$

We assume $Y$ given $X$ to be a line within that window (as evidenced by the linear equation).

## Plots

\

```{r first_loess}
total_days = diff(range(polls_2008$day))
span = 21/total_days

fit = loess(margin ~ day, degree = 1, span = span, data = polls_2008)

polls_2008 %>% 
  mutate(smooth = fit$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(alpha = 0.65, color = "grey") +
  geom_line(aes(day, smooth), color = "red")
```


Different `span` would give us different estimates.


```{r spans, echo=FALSE}
total_days = diff(range(polls_2008$day))
span = 21/total_days

fit_1 = loess(margin ~ day, degree = 1, span = 0.1, data = polls_2008)
fit_2 = loess(margin ~ day, degree = 1, span = 0.15, data = polls_2008)
fit_3 = loess(margin ~ day, degree = 1, span = 0.25, data = polls_2008)
fit_4 = loess(margin ~ day, degree = 1, span = 0.66, data = polls_2008)
```

```{r span_1, echo=FALSE}
polls_2008 %>% 
  mutate(smooth = fit_1$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(alpha = 0.65, color = "grey") +
  geom_line(aes(day, smooth), color = "red") +
  ggtitle("span = 0.1")
```


```{r span_2, echo=FALSE}
polls_2008 %>% 
  mutate(smooth = fit_2$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(alpha = 0.65, color = "grey") +
  geom_line(aes(day, smooth), color = "red") +
  ggtitle("span = 0.15")
```

```{r span_3, echo=FALSE}
polls_2008 %>% 
  mutate(smooth = fit_3$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(alpha = 0.65, color = "grey") +
  geom_line(aes(day, smooth), color = "red") +
  ggtitle("span = 0.25")
```

```{r span_4, echo=FALSE}
polls_2008 %>% 
  mutate(smooth = fit_4$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(alpha = 0.65, color = "grey") +
  geom_line(aes(day, smooth), color = "red") +
  ggtitle("span = 0.66")
```


## Three Differences Between Bin Smoothing and Loess

\

**The First Difference:**  
Rather than keeping the bin sizes the same (regardless of number of points within bin), **Loess keeps the number of points used in the local fit the same**. The width of this window is controlled by the `span` argument, which expects a proportion. For example: if $N$ is the number of total data points, and our `span` is equal to $0.5$, **the number of data points within _any_ given window will always be $0.5 \times N$ closest points to the fit**.

**The Second Difference:**  
When fitting a line locally, **Loess uses a weighted approach**. What this means is that instead of minimizing the Least Squares, **we must minimize a weighted version of Least Squares**. Represented mathematically like so:

$$
\sum_{i=1}^{N}w_0(x_i)\bigl[Y_i - \left\{\beta_0 + \beta_1(x_i - x_0)\right\}\bigr]^2
$$

Instead of using the Gaussian (normal) kernel for weighting, **Loess uses the Tukey Tri-Weight function**, expressed like this:

$$
\begin{align}
&W(u) = (1 - \vert{u}\vert^3)^3&\mathrm{\mathbf{if\ }} \vert{u}\vert \le1 \\
&\mathrm{\mathbf{and}} \\
&W(u) = 0&\mathrm{\mathbf{if\ }} \vert{u}\vert > 1
\end{align}
$$

And we use the following formula to actually define the weights:

$$
w_0(x_i)=W\Bigl(\frac{x_i - x_0}{h}\Bigr)
$$

**The Third Difference:**  
Loess offers the option to **fit the local model robustly**. What this means:

> "An iterative algorithm is implemented in which, after fitting a model in one iteration, **outliers** are detected and **down-weighted** for the next iteration."
- Dr. Rafael Irizarry

To use this robust option in `R`, we must set the `family` argument to `"symmetric"`.


## Important Note on Default Loess Behavior









