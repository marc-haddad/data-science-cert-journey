---
title: "Logistic Regression"
subtitle: "Machine Learning - Section $3.1.4$"
author: "_Marc Omar Haddad_"
date: "Published: 22 February, 2020 <br>"
output:
  html_notebook:
    theme: readable
    highlight: zenburn
  # pdf_document:
  #   latex_engine: xelatex
  #   df_print: kable
---
<!-- FOR UPDATES -->
  
  <!-- Uncomment if output = html_notebook: -->
    <!--  <center><strong>Updated: `r format(Sys.time(), '%d %B, %Y')`</strong></center> -->
  
  <!-- Uncomment if output = pdf_document: -->
    <!-- \begin{center}Updated: `r format(Sys.time(), '%d %B, %Y')`\end{center} -->

<!-- FOR UPDATES -->    

\
```{r dependencies, include=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(dslabs)
library(ggrepel)

# From prev. notes.rmd
data("heights")
y = heights$height
set.seed(2, sample.kind = "Rounding")

test_index = createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set = heights %>% slice(-test_index)
test_set = heights %>% slice(test_index)
lm_fit = mutate(train_set, y = as.numeric(sex == "Female")) %>% 
  lm(y ~ height, data = .)
p_hat = predict(lm_fit, test_set)
y_hat = ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
X = seq(60, 76, 1)

proportions = map_dbl(X, function(x) {
  train_set %>% 
    filter(height == round(x)) %>% 
    summarize(proportion = mean(sex == "Female")) %>% 
    .$proportion
})

```


**Logistic Regression** is an extension of linear regression that **ensures our estimated Conditional Probabilities are between $0$ and $1$**.

Logistic Regression requires a **logistic transformation** of our outcomes:
$$ g(p)=\mathrm{log} \frac{p}{1-p} $$

Logistic transformation converts *Probabilities* to *Log Odds*. **Log Odds** tell us how much more likely an event will happen versus *not* happen. For example: If $p = 0.5$ the odds are $1:1$.  
\

We use logistic transformation because it transforms probabilities to be symmetric around $0$.

\

```{r log_v_p, echo=FALSE, warning=FALSE}
df = data.frame(sequence = seq(0, 1, 0.01), log_val = log(seq(0, 1, 0.01)/(1 - seq(0,1,0.01))))
df %>% ggplot(aes(sequence, log_val)) +
  geom_line() +
  xlim(0.01, 0.99) +
  geom_hline(yintercept = 0, size = .3) +
  xlab("p") +
  ylab("log(p/(1-p))") +
  ggtitle("Logistic Transformation v. Probability")
```

To fit this model we use the **Maximum Likelihood Estimate (MLE)**. The `R` function `glm()` (which stands for "Generalized Linear Models") allows us to fit a logistic regression model.

```{r glm}
# The following fits a logistic regression model to our data.
glm_fit = train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>% 
  glm(y ~ height, data = ., family = "binomial") # Note: 'family' is required for glm().
```

Since `glm()` is more generalized than `lm()`, we are required to specify our desired model through the `family` parameter.  

\

Simalarly to our linear regression model we can obtain predictions with the `predict()` function.

```{r glm_predict}
p_hat_logit = predict(glm_fit, newdata = test_set, type = "response")
# 'type' param must be set to 'response' if we want conditional probs.
```

\













