---
title: "Regression for a Categorical Outcome"
subtitle: "Machine Learning - Section $3.1.3$"
author: "_Marc Omar Haddad_"
date: "Published: 22 February, 2020 <br>"
output:
  html_notebook:
    theme: readable
    highlight: zenburn
  # pdf_document:
  #   latex_engine: xelatex
  #   df_print: kable
---
<!-- FOR UPDATES -->
  
  <!-- Uncomment if output = html_notebook: -->
     <center><strong>Updated: `r format(Sys.time(), '%d %B, %Y')`</strong></center>
  
  <!-- Uncomment if output = pdf_document: -->
    <!-- \begin{center}Updated: `r format(Sys.time(), '%d %B, %Y')`\end{center} -->

<!-- FOR UPDATES -->    

\
```{r dependencies, include=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(dslabs)
library(ggrepel)
data("heights")
y = heights$height
set.seed(2, sample.kind = "Rounding")

test_index = createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set = heights %>% slice(-test_index)
test_set = heights %>% slice(test_index)

height_seq2 = seq(60, 76, 0.01) # Define new height_seq w/ small intervals.
glm_fit2 = train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>% 
  glm(y ~ height, data = ., family = "binomial") 
height_seq2 = seq(60, 76, 0.01)
p_hat_logit2 = predict(glm_fit2, list(height = height_seq2), type = "response")

lm_fit = mutate(train_set, y = as.numeric(sex == "Female")) %>% 
  lm(y ~ height, data = .)

# Re-predict cond. probs. for each reg. type w/ new height_seq.
p_hat_logit2 = predict(glm_fit2, list(height = height_seq2), type = "response")
p_hat2 = predict(lm_fit, list(height = height_seq2))

df_wide = data.frame(height = height_seq2, logistic = p_hat_logit2, linear = p_hat2)

# Convert from wide-form to long-form data for ggplot.
df_long = gather(df_wide, reg_type, reg_val, logistic:linear, factor_key = TRUE)
```

The regression approach discussed in previous lessons can also be applied to Categorical Data.  
```{r init, warning=FALSE, results='hide'}
data("heights")
y = heights$height
set.seed(2, sample.kind = "Rounding")

test_index = createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set = heights %>% slice(-test_index)
test_set = heights %>% slice(test_index)

```


We will define the outcome as $Y = 1$ for `female`, and $Y = 0$ for `male`, and with feature $X =$`height`. With this definition we are interested in the **Conditional Probability of being `female` when given `height`**; represented mathematically as:
$$ \mathrm{Pr}(Y = 1\ |\ X = x) $$

So, we ask ourselves: *What is the **conditional probability of being female** if you are **66 inches** tall? *

We can calculate the probability by simply rounding `height` entries that are near $66$ inches to $66$, and then calculating the proportion of females.

```{r fem_prop}
train_set %>% 
  filter(height == round(66)) %>% 
  summarize(`Conditional Prob. of being Female` = mean(sex == "Female"))
```

The Conditional Probability of being `female` given a `height` of $66$ inches is $21.4$%.

\newpage

Now we will repeat the same exercise for multiple values of $X$.  

```{r multi_X_prob}
X = seq(60, 76, 1)

proportions = map_dbl(X, function(x) {
  train_set %>% 
    filter(height == round(x)) %>% 
    summarize(proportion = mean(sex == "Female")) %>% 
    .$proportion
})

qplot(X, proportions)

```

Since the results of the above plot appear to be linear, we can try regression.

Reminder: When using regression we assume that **Conditional Probability** can be *expressed* as a **linear function**:
$$ p(x) =\ \mathrm{Pr}(Y=1\ |\ X=x)\ = \beta_0 + \beta_1x $$

We plug in our values into the `lm()` function to yield an estimate of $\beta_0$ and $\beta_1$.

```{r betas}
lm_fit = mutate(train_set, y = as.numeric(sex == "Female")) %>% 
  lm(y ~ height, data = .)
lm_fit$coefficients
```

Now that we have our estimates, we can extract an actual prediction. The estimate of our Conditional Probability is:
$$ \hat{p}(x)=\hat{\beta_0} + \hat{\beta_1}x $$

We can visualize our **Conditional Expectations / Conditional Probabilities** by plugging the values of `lm_fit$coefficients` into the above function and plotting the resulting line.

```{r linear_plot, echo=FALSE}
plot(X, proportions, pch = 16)
lines(height_seq2, p_hat2, lty = 2)
```



Using this formula, we form a prediction by defining a **Decision Rule**. In this case our Decision Rule is:  


**Predict `female` if \ $\hat{p}(x) > 0.5$**. 


We can then use the `confusionMatrix()` function to assess our model.

```{r lm_prediction}
p_hat = predict(lm_fit, test_set)
y_hat = ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)
```










